import json
import os
import boto3
import psycopg2
from io import BytesIO
import sys
sys.path.insert(0, '/function/code/shared')
from auth_middleware import get_tenant_id_from_request
sys.path.append('/function/code')
from api_keys_helper import get_tenant_api_key
from token_logger import log_token_usage
from timezone_helper import moscow_naive

def handler(event: dict, context) -> dict:
    """–û–±—Ä–∞–±–æ—Ç–∫–∞ PDF: –∏–∑–≤–ª–µ—á–µ–Ω–∏–µ —Ç–µ–∫—Å—Ç–∞, —Ä–∞–∑–±–∏–µ–Ω–∏–µ –Ω–∞ —á–∞–Ω–∫–∏ –∏ —Å–æ–∑–¥–∞–Ω–∏–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤"""
    method = event.get('httpMethod', 'POST')

    if method == 'OPTIONS':
        return {
            'statusCode': 200,
            'headers': {
                'Access-Control-Allow-Origin': '*',
                'Access-Control-Allow-Methods': 'POST, OPTIONS',
                'Access-Control-Allow-Headers': 'Content-Type, Authorization, X-Authorization'
            },
            'body': '',
            'isBase64Encoded': False
        }

    if method != 'POST':
        return {
            'statusCode': 405,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': 'Method not allowed'}),
            'isBase64Encoded': False
        }

    try:
        print(f"üîç DEBUG process-pdf: headers={event.get('headers', {})}, queryParams={event.get('queryStringParameters', {})}, body={event.get('body', '{}')}")
        tenant_id, auth_error = get_tenant_id_from_request(event)
        if auth_error:
            print(f"‚ùå AUTH ERROR in process-pdf: {auth_error}")
            return auth_error
        print(f"‚úÖ AUTH SUCCESS in process-pdf: tenant_id={tenant_id}")
        
        import PyPDF2
        from openai import OpenAI
        
        body = json.loads(event.get('body', '{}'))
        document_id = body.get('documentId')

        if not document_id:
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': 'documentId required'}),
                'isBase64Encoded': False
            }

        conn = psycopg2.connect(os.environ['DATABASE_URL'])
        conn.autocommit = True
        cur = conn.cursor()
        
        print(f"üîç SEARCHING FOR DOCUMENT: document_id={document_id}, tenant_id={tenant_id}")
        cur.execute("SELECT file_key, tenant_id FROM t_p56134400_telegram_ai_bot_pdf.tenant_documents WHERE id = %s AND tenant_id = %s", (document_id, tenant_id))
        result = cur.fetchone()
        print(f"üìä QUERY RESULT: {result}")
        
        if not result:
            print(f"‚ùå Document not found: document_id={document_id}, tenant_id={tenant_id}")
            cur.close()
            conn.close()
            return {
                'statusCode': 404,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': f'Document not found: id={document_id}, tenant={tenant_id}'}),
                'isBase64Encoded': False
            }

        file_key = result[0]

        s3 = boto3.client('s3',
            endpoint_url='https://bucket.poehali.dev',
            aws_access_key_id=os.environ['AWS_ACCESS_KEY_ID'],
            aws_secret_access_key=os.environ['AWS_SECRET_ACCESS_KEY']
        )
        
        print(f"üì¶ TRYING TO GET FILE FROM S3: Bucket='files', Key='{file_key}'")
        try:
            obj = s3.get_object(Bucket='files', Key=file_key)
            pdf_data = obj['Body'].read()
            print(f"‚úÖ FILE DOWNLOADED FROM S3: {len(pdf_data)} bytes")
        except Exception as s3_error:
            print(f"‚ùå S3 ERROR: {s3_error}")
            cur.close()
            conn.close()
            return {
                'statusCode': 500,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': f'S3 error: {str(s3_error)}'}),
                'isBase64Encoded': False
            }

        print(f"üìñ PARSING PDF: {len(pdf_data)} bytes")
        pdf_reader = PyPDF2.PdfReader(BytesIO(pdf_data))
        pages_count = len(pdf_reader.pages)
        print(f"üìÑ PDF HAS {pages_count} PAGES")
        
        if pages_count > 20:
            cur.close()
            conn.close()
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': f'PDF —Å–ª–∏—à–∫–æ–º –±–æ–ª—å—à–æ–π: {pages_count} —Å—Ç—Ä–∞–Ω–∏—Ü. –ú–∞–∫—Å–∏–º—É–º: 20 —Å—Ç—Ä–∞–Ω–∏—Ü'}),
                'isBase64Encoded': False
            }
        
        print(f"üî§ EXTRACTING TEXT FROM {pages_count} PAGES...")
        full_text = ""
        for page in pdf_reader.pages:
            full_text += page.extract_text() + "\n\n"
        print(f"‚úÖ TEXT EXTRACTED: {len(full_text)} chars")

        chunk_size = 1000
        chunks = []
        for i in range(0, len(full_text), chunk_size):
            chunk = full_text[i:i + chunk_size]
            if chunk.strip():
                chunks.append(chunk)
        print(f"‚úÇÔ∏è CREATED {len(chunks)} CHUNKS")
        
        if len(chunks) > 200:
            cur.close()
            conn.close()
            return {
                'statusCode': 400,
                'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
                'body': json.dumps({'error': f'–°–ª–∏—à–∫–æ–º –º–Ω–æ–≥–æ —Ç–µ–∫—Å—Ç–∞: {len(chunks)} —Ñ—Ä–∞–≥–º–µ–Ω—Ç–æ–≤. –ú–∞–∫—Å–∏–º—É–º: 200'}),
                'isBase64Encoded': False
            }

        # –ü–æ–ª—É—á–∞–µ–º –Ω–∞—Å—Ç—Ä–æ–π–∫–∏ —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤ –î–û —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏
        cur.execute("""
            SELECT embedding_provider, embedding_doc_model
            FROM t_p56134400_telegram_ai_bot_pdf.tenant_settings
            WHERE tenant_id = %s
        """, (tenant_id,))
        settings_row = cur.fetchone()
        
        embedding_provider = settings_row[0] if settings_row and settings_row[0] else 'yandex'
        embedding_doc_model = settings_row[1] if settings_row and settings_row[1] else 'text-search-doc'
        print(f"‚öôÔ∏è EMBEDDING SETTINGS: provider={embedding_provider}, model={embedding_doc_model}")
        
        import requests
        
        # –ü–æ–ª—É—á–∞–µ–º API –∫–ª—é—á–∏ –î–û —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–í–°–ï–ì–î–ê –∏—Å–ø–æ–ª—å–∑—É–µ–º PROJECT —Å–µ–∫—Ä–µ—Ç—ã –¥–ª—è —ç–º–±–µ–¥–¥–∏–Ω–≥–æ–≤)
        yandex_api_key = None
        yandex_folder_id = None
        if embedding_provider == 'yandex':
            yandex_api_key = os.environ.get('YANDEXGPT_API_KEY')
            yandex_folder_id = os.environ.get('YANDEXGPT_FOLDER_ID')
            print(f"üîë PROJECT KEYS: api_key={'‚úÖ found' if yandex_api_key else '‚ùå missing'}, folder_id={'‚úÖ found' if yandex_folder_id else '‚ùå missing'}")
            if not yandex_api_key or not yandex_folder_id:
                print(f"‚ö†Ô∏è No PROJECT Yandex API keys found, skipping embeddings")
                yandex_api_key = None
                yandex_folder_id = None
        
        # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º –≤—Å–µ —ç–º–±–µ–¥–¥–∏–Ω–≥–∏ –î–û —Ç—Ä–∞–Ω–∑–∞–∫—Ü–∏–∏ (–µ—Å–ª–∏ –µ—Å—Ç—å API –∫–ª—é—á–∏)
        import time
        import re
        print(f"üöÄ STARTING EMBEDDING GENERATION for {len(chunks)} chunks...")
        
        # –§—É–Ω–∫—Ü–∏—è –¥–ª—è –æ–±–æ–≥–∞—â–µ–Ω–∏—è —Ç–µ–∫—Å—Ç–∞ –¥–∞—Ç–∞–º–∏ –∏–∑ –ø–µ—Ä–∏–æ–¥–æ–≤
        def enrich_with_dates(text):
            """–î–æ–±–∞–≤–ª—è–µ—Ç —è–≤–Ω—ã–µ —É–ø–æ–º–∏–Ω–∞–Ω–∏—è –¥–∞—Ç –¥–ª—è –ø–µ—Ä–∏–æ–¥–æ–≤ –≤ —Ñ–æ—Ä–º–∞—Ç–µ DD.MM.YYYY-DD.MM.YYYY"""
            # –ü–∞—Ç—Ç–µ—Ä–Ω –¥–ª—è –ø–æ–∏—Å–∫–∞ –ø–µ—Ä–∏–æ–¥–æ–≤ —Ç–∏–ø–∞ "01.03.2026-31.03.2026"
            period_pattern = r'(\d{2})\.(\d{2})\.(\d{4})-(\d{2})\.(\d{2})\.(\d{4})'
            matches = re.findall(period_pattern, text)
            
            if not matches:
                return text
            
            enriched = text
            month_names = {
                '01': '—è–Ω–≤–∞—Ä—è', '02': '—Ñ–µ–≤—Ä–∞–ª—è', '03': '–º–∞—Ä—Ç–∞', '04': '–∞–ø—Ä–µ–ª—è',
                '05': '–º–∞—è', '06': '–∏—é–Ω—è', '07': '–∏—é–ª—è', '08': '–∞–≤–≥—É—Å—Ç–∞',
                '09': '—Å–µ–Ω—Ç—è–±—Ä—è', '10': '–æ–∫—Ç—è–±—Ä—è', '11': '–Ω–æ—è–±—Ä—è', '12': '–¥–µ–∫–∞–±—Ä—è'
            }
            month_names_nom = {
                '01': '—è–Ω–≤–∞—Ä—å', '02': '—Ñ–µ–≤—Ä–∞–ª—å', '03': '–º–∞—Ä—Ç', '04': '–∞–ø—Ä–µ–ª—å',
                '05': '–º–∞–π', '06': '–∏—é–Ω—å', '07': '–∏—é–ª—å', '08': '–∞–≤–≥—É—Å—Ç',
                '09': '—Å–µ–Ω—Ç—è–±—Ä—å', '10': '–æ–∫—Ç—è–±—Ä—å', '11': '–Ω–æ—è–±—Ä—å', '12': '–¥–µ–∫–∞–±—Ä—å'
            }
            
            for match in matches:
                start_day, start_month, start_year, end_day, end_month, end_year = match
                
                # –ì–µ–Ω–µ—Ä–∏—Ä—É–µ–º —Å–ø–∏—Å–æ–∫ –¥–∞—Ç (–∫–∞–∂–¥—ã–π –¥–µ–Ω—å –≤ –ø–µ—Ä–∏–æ–¥–µ)
                dates_list = []
                dates_list.append(f"{month_names_nom[start_month]} {start_year}")
                
                # –ï—Å–ª–∏ –ø–µ—Ä–∏–æ–¥ –≤ –ø—Ä–µ–¥–µ–ª–∞—Ö –æ–¥–Ω–æ–≥–æ –º–µ—Å—è—Ü–∞ ‚Äî –¥–æ–±–∞–≤–ª—è–µ–º –≤—Å–µ –¥–∞—Ç—ã
                if start_month == end_month and start_year == end_year:
                    for day in range(1, int(end_day) + 1):
                        dates_list.append(f"{day} {month_names[start_month]}")
                else:
                    # –ü–µ—Ä–∏–æ–¥ —á–µ—Ä–µ–∑ –Ω–µ—Å–∫–æ–ª—å–∫–æ –º–µ—Å—è—Ü–µ–≤
                    dates_list.append(f"{month_names_nom[end_month]} {end_year}")
                    # –î–æ–±–∞–≤–ª—è–µ–º –ø—Ä–∏–º–µ—Ä –¥–∞—Ç –∏–∑ –Ω–∞—á–∞–ª–∞ –∏ –∫–æ–Ω—Ü–∞
                    for day in [1, 5, 10, 15, 20, 25, int(end_day)]:
                        if day <= int(end_day):
                            dates_list.append(f"{day} {month_names[end_month]}")
                
                # –î–æ–±–∞–≤–ª—è–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç
                dates_text = ", ".join(dates_list)
                enriched += f"\n\n–î–∞—Ç—ã –≤ —ç—Ç–æ–º –ø–µ—Ä–∏–æ–¥–µ: {dates_text}"
                break  # –û–±—Ä–∞–±–∞—Ç—ã–≤–∞–µ–º —Ç–æ–ª—å–∫–æ –ø–µ—Ä–≤—ã–π –ø–µ—Ä–∏–æ–¥ –≤ chunk
            
            return enriched
        
        chunk_embeddings = []
        for idx, chunk_text in enumerate(chunks):
            # –û–±–æ–≥–∞—â–∞–µ–º —Ç–µ–∫—Å—Ç –¥–∞—Ç–∞–º–∏ –ü–ï–†–ï–î —Å–æ–∑–¥–∞–Ω–∏–µ–º embedding
            embedding_text = enrich_with_dates(chunk_text)
            embedding_json = None
            try:
                if embedding_provider == 'yandex' and yandex_api_key and yandex_folder_id:
                    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è embedding
                    emb_response = requests.post(
                        'https://llm.api.cloud.yandex.net/foundationModels/v1/textEmbedding',
                        headers={
                            'Authorization': f'Api-Key {yandex_api_key}',
                            'Content-Type': 'application/json'
                        },
                        json={
                            'modelUri': f'emb://{yandex_folder_id}/{embedding_doc_model}/latest',
                            'text': embedding_text
                        },
                        timeout=30
                    )
                    if emb_response.status_code != 200:
                        print(f"‚ùå YANDEX API ERROR for chunk {idx}: {emb_response.status_code}, {emb_response.text}")
                        raise Exception(f"Yandex API error: {emb_response.status_code}")
                    
                    emb_data = emb_response.json()
                    if 'embedding' not in emb_data:
                        print(f"‚ùå NO 'embedding' in response for chunk {idx}: {emb_data}")
                        raise Exception(f"Missing 'embedding' in response")
                    
                    embedding_vector = emb_data['embedding']
                    embedding_json = json.dumps(embedding_vector)
                    
                    if (idx + 1) % 5 == 0:
                        print(f"‚úÖ Processed {idx + 1}/{len(chunks)} chunks")
                    
                    # –õ–æ–≥–∏—Ä—É–µ–º –∏—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ —Ç–æ–∫–µ–Ω–æ–≤ (–ø—Ä–∏–º–µ—Ä–Ω–æ 256 —Ç–æ–∫–µ–Ω–æ–≤ –Ω–∞ chunk)
                    tokens_estimate = min(len(chunk_text) // 4, 256)
                    log_token_usage(
                        tenant_id=tenant_id,
                        operation_type='embedding_create',
                        model=embedding_doc_model,
                        tokens_used=tokens_estimate,
                        metadata={'document_id': document_id, 'chunk_index': idx}
                    )
                    
                    if (idx + 1) % 10 == 0:
                        time.sleep(0.5)
                else:
                    if idx == 0:
                        print(f"Embeddings disabled: provider={embedding_provider}, has_key={bool(yandex_api_key)}")
            except Exception as emb_error:
                print(f"‚ùå Embedding error for chunk {idx}: {emb_error}")
                import traceback
                traceback.print_exc()
                embedding_json = None
            
            # –°–æ—Ö—Ä–∞–Ω—è–µ–º –û–†–ò–ì–ò–ù–ê–õ–¨–ù–´–ô chunk_text –∏ –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π embedding_text –æ—Ç–¥–µ–ª—å–Ω–æ
            chunk_embeddings.append((chunk_text, embedding_text, embedding_json))
        
        print(f"‚úÖ EMBEDDING GENERATION COMPLETE: {len(chunk_embeddings)} chunks processed")

        # –û–ø–µ—Ä–∞—Ü–∏–∏ —Å —á–∞–Ω–∫–∞–º–∏ (—É–¥–∞–ª–µ–Ω–∏–µ + –≤—Å—Ç–∞–≤–∫–∞)
        print(f"üíæ STARTING CHUNKS OPERATIONS...")
        try:
            # –£–¥–∞–ª—è–µ–º —Å—Ç–∞—Ä—ã–µ —á–∞–Ω–∫–∏
            cur.execute("DELETE FROM t_p56134400_telegram_ai_bot_pdf.document_chunks WHERE document_id = %s", (document_id,))
            cur.execute("DELETE FROM t_p56134400_telegram_ai_bot_pdf.tenant_chunks WHERE document_id = %s", (document_id,))
            print(f"üóëÔ∏è Deleted old chunks for document_id={document_id}")
            
            # –í—Å—Ç–∞–≤–ª—è–µ–º –≤—Å–µ –Ω–æ–≤—ã–µ —á–∞–Ω–∫–∏
            for idx, (chunk_text, enriched_text, embedding_json) in enumerate(chunk_embeddings):
                # –í document_chunks —Å–æ—Ö—Ä–∞–Ω—è–µ–º –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π chunk_text
                cur.execute("""
                    INSERT INTO t_p56134400_telegram_ai_bot_pdf.document_chunks 
                    (document_id, chunk_text, chunk_index, embedding_text)
                    VALUES (%s, %s, %s, %s)
                """, (document_id, chunk_text, idx, embedding_json))
                
                # –í tenant_chunks —Å–æ—Ö—Ä–∞–Ω—è–µ–º:
                # - chunk_text: –æ—Ä–∏–≥–∏–Ω–∞–ª—å–Ω—ã–π —Ç–µ–∫—Å—Ç –¥–ª—è –ø–æ–∫–∞–∑–∞ –ø–æ–ª—å–∑–æ–≤–∞—Ç–µ–ª—é
                # - enriched_text: –æ–±–æ–≥–∞—â–µ–Ω–Ω—ã–π —Ç–µ–∫—Å—Ç —Å –¥–∞—Ç–∞–º–∏ (–∏—Å–ø–æ–ª—å–∑—É–µ—Ç—Å—è –¥–ª—è embedding)
                # - embedding_text: JSON –≤–µ–∫—Ç–æ—Ä (—Ä–∞—Å—Å—á–∏—Ç–∞–Ω –Ω–∞ –æ—Å–Ω–æ–≤–µ enriched_text)
                cur.execute("""
                    INSERT INTO t_p56134400_telegram_ai_bot_pdf.tenant_chunks 
                    (tenant_id, document_id, chunk_text, chunk_index, embedding_text, enriched_text)
                    VALUES (%s, %s, %s, %s, %s, %s)
                """, (tenant_id, document_id, chunk_text, idx, embedding_json, enriched_text))
            
            print(f"üìù Inserted {len(chunk_embeddings)} chunks into database")
            
        except Exception as chunks_error:
            print(f"‚ùå CHUNKS ERROR: {chunks_error}")
            import traceback
            traceback.print_exc()
            cur.close()
            conn.close()
            raise chunks_error
        
        # –ó–∞–∫—Ä—ã–≤–∞–µ–º –ø–µ—Ä–≤–æ–µ —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ
        cur.close()
        conn.close()
        print(f"üîå Closed first connection after chunks")
        
        # –û—Ç–∫—Ä—ã–≤–∞–µ–º –ù–û–í–û–ï —Å–æ–µ–¥–∏–Ω–µ–Ω–∏–µ –¥–ª—è UPDATE –¥–æ–∫—É–º–µ–Ω—Ç–∞
        print(f"üîå Opening NEW connection for document UPDATE...")
        conn2 = psycopg2.connect(os.environ['DATABASE_URL'])
        conn2.autocommit = True
        cur2 = conn2.cursor()
        
        try:
            # –û–±–Ω–æ–≤–ª—è–µ–º —Å—Ç–∞—Ç—É—Å –¥–æ–∫—É–º–µ–Ω—Ç–∞
            print(f"üìù Updating document status: doc_id={document_id}, pages={pages_count}")
            
            cur2.execute("""
                UPDATE t_p56134400_telegram_ai_bot_pdf.tenant_documents 
                SET status = 'ready', pages = %s
                WHERE id = %s
                RETURNING id, status
            """, (pages_count, document_id))
            updated_doc = cur2.fetchone()
            print(f"‚úÖ Updated document status to 'ready': {updated_doc}")
            
            print(f"‚úÖ ALL OPERATIONS COMPLETED SUCCESSFULLY")
            
        except Exception as update_error:
            print(f"‚ùå UPDATE ERROR: {update_error}")
            import traceback
            traceback.print_exc()
            raise update_error
        finally:
            cur2.close()
            conn2.close()
            print(f"üîå Closed second connection")

        return {
            'statusCode': 200,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({
                'documentId': document_id,
                'pages': pages_count,
                'chunks': len(chunks),
                'status': 'ready'
            }),
            'isBase64Encoded': False
        }

    except Exception as e:
        return {
            'statusCode': 500,
            'headers': {'Content-Type': 'application/json', 'Access-Control-Allow-Origin': '*'},
            'body': json.dumps({'error': str(e)}),
            'isBase64Encoded': False
        }